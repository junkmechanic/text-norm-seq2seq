README for Normalization System using Sequence to Sequence Learning

By Ankur Khanna (ankur)

Following is the description of the system build by me for Normalization of
Noisy User-generated text (like tweets, youtube comments etc).

All the source code files are version controlled using git. First, let's look at
some of the important files:

data/
    train_data.json                     -- benchmark training data
    test_truth.json                     -- benchmark test data
    clear_tweets.json                   -- tweets with no OOVs
trans_norm_train.py
# tf_normalize.py
tf_normalize_merge.py
tf_normalize_skel.py

The main script to train the mold is './trans_norm_model.py'. If you supply it
with the command line flag '--test' then it will run the model on the test set.

The script './tf_normalize_skel.py' is used to run the model on the test set and
create an output file that would be used by 'tf_normalize_merge.py'. The suffix
'skel' stands for skeleton because it does not run the entire normalization
system but only the prediction part where the deep learning model is used.

The script './tf_normalize_merge.py' then uses the output of the skeleton script
to merge it with the rule based modules.

The reason why the two modules (prediciton and rule-based) are run separately is
becasue the prediction module takes a long time to load the model and then
predict the output. And often, you may want to make a small change in the rule-
based module and check the change in the output. Hence, it is convenient to just
save the output of the prediction as the skeleton output and then run the rule-
based module on top of it.

All other files are older versions that were used for either inspiration
(shamelessly copying) or running/testing/tuning the model.

To run the scripts, the first thing you need to do is to change the directory
name in the PATHS dictionary for the key 'root'. It should properly reflect the
full name of the root of this library.

Following is some information on individual scripts:

trans_norm_train.py
    This script trains the sequence to sequence model for normalization. The
    first thing it does is preprocess the data from the train and test files to
    convert it into a binary format which is easier to feed to TensorFlow. You
    can change the options to change the way the data is preprocessed. To do
    that, change the arguments given to the 'DataSource' object during
    initialization. You can refer to the class description in 'dataUtils.py'
    for more details on all the options available.

    Next, the learning model is initialized. You can tune the model parameters
    by changing the values given to the 'TransNormModel' object. I would advise
    you to change these values in the VARS dictionary in 'utilities.py'.

    Depending on whether the commandline option '--test' was supplied, the model
    is either trained or used for prediction on test set. The test set prediction
    is on a per word basis including words in the vocabulary. So it should not
    be a surprise that it is a high value. If you want to check the actual
    performance of the system for Normalization, you should run the scripts
    'tf_normalize_skel' and 'tf_normalize_merge' as explained earlier.

    The option '--test' is used to indicate that the model should use the
    provided test files and run prediction on them. So you can replace the files
    with development set files too.
